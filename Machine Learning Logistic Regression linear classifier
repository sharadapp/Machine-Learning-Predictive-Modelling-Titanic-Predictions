""" Titanic Survival rate predictions using the Logistic Regression Linear Classifier"""

# reset-f

# Importing the libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset

dataset_train = pd.read_csv('Training_Data_Set_Titanic.csv')
dataset_test = pd.read_csv('Test_Data_Set_Titanic.csv')
X_train = dataset_train.iloc[:, [0,2,4,5,6,7,9]].values
y_train = dataset_train.iloc[:, 1].values
dataset_test.fillna(-99999, inplace=True)
X_test = dataset_test.iloc[:, [0,1,3,4,5,6,8]].values
passenger_id=X_test[:,0]

# Taking care of missing data for numerical values in training dataset

from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
imputer = imputer.fit(X_train[:, [3]])
X_train[:,[3]] = imputer.transform(X_train[:,[3]])

# Taking care of missing data for numerical values in test dataset

from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)
imputer = imputer.fit(X_test[:, [3]])
X_test[:,[3]] = imputer.transform(X_test[:,[3]])

# Encoding categorical data
# Encoding the Independent Variable of the Training dataset

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_Xtrain = LabelEncoder()
X_train[:,2] = labelencoder_Xtrain.fit_transform(X_train[:,2])
onehotencoder = OneHotEncoder(categorical_features =[2])
X_train = onehotencoder.fit_transform(X_train).toarray()

# Encoding categorical data
# Encoding the Independent Variable of the Test dataset

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_Xtest = LabelEncoder()
X_test[:,2] = labelencoder_Xtest.fit_transform(X_test[:,2])
onehotencoder = OneHotEncoder(categorical_features =[2])
X_test = onehotencoder.fit_transform(X_test).toarray()

# Feature Scaling

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

# to get rid of the infinite ;value error and to check if the input n-d array has  null values and values greater than float64 use
#these commands
"""np.isnan(X_test.any());np.any(np.isnan(X_test))
np.isfinite(X_test.all());np.all(np.isfinite(X_test))"""

# Applying Kernel _pca on a non-milear problem for getting a smother prediction boundary to capture all the real observation data points.

from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components = 2, kernel = "rbf")
X_train = kpca.fit_transform(X_train)
X_test = kpca.transform(X_test)

# Fitting the Logistic Regression Model

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train,y_train)

#Predicting the test set results

y_pred = classifier.predict(X_train)

#To store the prediction results in a csv file.
"""results={"PassengerID":passenger_id,"Survived":y_pred}
results_df=pd.DataFrame(data=y_pred,index=passenger_id,columns=["Survived"])
results_df.to_csv("titanic_pred_logistic_regg.csv")"""

# Visualising the training set results in Logistic Regression Model after applying the Dimensionality reduction Technique.

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Applying the K-Fold Cross Validation for right model selection technique and for evaluating the model performance

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier,X = X_train,y = y_train,cv = 10)
accuracies.mean()
accuracies.std()
